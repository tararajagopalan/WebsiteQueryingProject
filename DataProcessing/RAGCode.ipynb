{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tararajagopalan/WebsiteQueryingProject/blob/main/RAGCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FP8sKyjPqW4"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4LGMcDoQVY7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3XOjO-_QcF8"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "#installing api key in secrets and getting it from there\n",
        "#openai.api_key = userdata.get('openapikey') # This line is not needed\n",
        "\n",
        "#creating open ai client with key from secrets\n",
        "openai_client = OpenAI(api_key=userdata.get('openapikey'),)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzJ5etAJaT0W"
      },
      "outputs": [],
      "source": [
        "#milvus installations\n",
        "\n",
        "!pip install -U pymilvus\n",
        "!pip install --upgrade pymilvus\n",
        "!pip install \"pymilvus[model]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c153537"
      },
      "outputs": [],
      "source": [
        "#enabling google drive access to notebook\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7ydd9yWarry"
      },
      "outputs": [],
      "source": [
        "#SETTING UP VECTOR DATA BASE: creating a database on the mounted google drive\n",
        "#sets up data base in the milvus_demo.db file on google drive\n",
        "from pymilvus import MilvusClient\n",
        "\n",
        "milvus_client = MilvusClient(\"/content/drive/MyDrive/project_work.db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBkk7QeaaYC7"
      },
      "outputs": [],
      "source": [
        "milvus_client.describe_collection(\"project_collection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8bm2-Ena4xr"
      },
      "outputs": [],
      "source": [
        "from pymilvus import model\n",
        "\n",
        "embedding_fn = model.DefaultEmbeddingFunction()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcsPmyvLeKgA"
      },
      "outputs": [],
      "source": [
        "#question of interest to query chat gpt\n",
        "\n",
        "question = \"How to train a dog?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpinwfwRa9HL"
      },
      "outputs": [],
      "source": [
        "#placing question into query_vectors to be embedded and specifying search_params here\n",
        "\n",
        "query_vectors = embedding_fn.encode_queries([question])\n",
        "\n",
        "\n",
        "search_params = {\n",
        "    \"metric_type\": \"L2\",          # or \"IP\" depending on your index\n",
        "    \"params\": {\"nprobe\": 10}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_puVvgRpbBsK"
      },
      "outputs": [],
      "source": [
        "#querying the project_collection milvus database according to the question of interest\n",
        "\n",
        "search_res = milvus_client.search(\n",
        "    collection_name=\"project_collection\",\n",
        "    data=query_vectors,\n",
        "    anns_field=\"embedding\",\n",
        "    search_params = search_params,\n",
        "    limit=5, #returns top 2 similar results\n",
        "    output_fields=[\"text\",\"FilePath\",\"FileExtension\"], #this is what you select -> in SQL: select text, subject from demo_collection where vector is similar to data\n",
        "\n",
        ")\n",
        "\n",
        "#2 relevant entries to the query are basically printed below\n",
        "print(search_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vz_xfBJCctBI"
      },
      "outputs": [],
      "source": [
        "#this is basically showing us the results of the search query which was serialized into a json formatting string\n",
        "import json\n",
        "\n",
        "retrieved_lines_with_distances = [\n",
        "    (res[\"entity\"][\"text\"], res[\"distance\"]) for res in search_res[0]\n",
        "]\n",
        "print(json.dumps(retrieved_lines_with_distances, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6N63SVuc-DJ"
      },
      "outputs": [],
      "source": [
        "#Using LLM to get a RAG Response\n",
        "\n",
        "#converting retrieved documents into string format\n",
        "context = \"\\n\".join(\n",
        "    [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMqwcLnAdav4"
      },
      "outputs": [],
      "source": [
        "#define system and user prompts for Lanage Model. Prompt is assembled with retrieved documents from milvus\n",
        "\n",
        "#SYSTEM_PROMPT: instructs the system on how to behave\n",
        "#USER_PROMPT: basically is saying to use context of the retrieved documents from the milvus query to appropriately answer the question of interest\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
        "\"\"\"\n",
        "USER_PROMPT = f\"\"\"\n",
        "Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Kg00745eqX2"
      },
      "outputs": [],
      "source": [
        "#Use OpenAI ChatGPT to generate a response based on the prompts\n",
        "\n",
        "response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": USER_PROMPT},\n",
        "    ],\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
